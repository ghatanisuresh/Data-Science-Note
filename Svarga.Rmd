---
date: "2024-07-15"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## What are Artificial Intelligence, Machine Learning, Deep Learning and Generative AI? 

Most people often get confused over these three terms. To begin with, Artificial intelligence refers to techniques, methodologies, and algorithms that includes those of Machine Learning, Deep Learning, and generative AI. In a general overview, AI equip computers to simulate human behaviour, enabling them to learn, make decisions, recognize patterns, and solve complex problems in a manner like human intelligence. Therefore, it's objective is to create a intelligent agents, which get the percpetion of the real world through the using of sensors, learn using some of the advanced ML tecniques, make decisions and finally implement through actuators. 

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("D:\\Personal Work\\Note\\Data Science\\Overview.png")
```
<center> Hierrachical Relationship</center>

__Machine Learning:__ It is a subset of the AI which is the science of programming a machine to make it learn to perform a task from the data, rather than through programming. 

__Deep Learning:__ It is again the subset of the AI, but also further subset of the Machine Learning field which is an ensemble of tecniques and alogorithm that exploit neural networks for in depth data processing and analytical tasks. And it involves multiple layers of artificial neural netowkrks to extract high-level of features from raw input data, simulating the way human brains, percieve and understand the world. 

__Generative AI:__ This is again the subset of deep learning models which is more advanced form of AI, that has the capacity to generate content like text, images, or code based on provided input.


Short and Suit example: 

__An Autonomous car__:

Artificial in this system, collects data from real world using sensors(road, cars, road signs, speed, objects), possesses a set of rules of behaviour that has its intelligence embeded. Some of the intelligence are: If a car is about to collide with an object, brake preventively, with the rules: a set of actuators implementing relative decisions(brakes, steering, ABS).

In this AI system of the car has a sub module of Machine Learning that allows it to improve over time, as the car "sees" different roads and situtations and increase its performance compared to some metric(safety, reaction time to braking, consumption).

Addditionally, sub-modules can use Deep Learning techniques such as neural networks, trained to recognize the route of the road, the shapers of cars, distinguish them from those of trees and pedestrians, and so on. 


## Traditional vs Modern Programming: Machine Learning

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("D:\\Personal Work\\Note\\Data Science\\approach.jpg")
```


Traditionally, the classic approach to the problem consists in designing a process(algorithm) that is then realized in a programming language implmentation), thus solving the problem in a deterministic fashion.

The approach of Machine Learning consists instead in seeing a lot of input examples with the output considered correct, and programming the machine to detect the recurring patterns that occur in the data. 
So, what happens is, in this ML method, there  involves corrected input and output together, and programmed the machine to find the recurring patterns that occur in a dataset/business model. Then pattern is used by machine as "knowledge", and according to that knowledge, machine tries to return the correct output provided new input(seen or unseen).

And here comes the important terms, __training__, which is the process of feeding with data and its result is a model: a set of parameters properly 'adjusted' so they could recongnize the patterns that interest us for the given problem. Once a model is obtained, it can be used to predict the correct output to inputs that it has never "seen". 

Some of the great examples of using Machine Learning is applied in the real world: 

1. Spam email filter. 
2. Hand drawn figures or symbols, such as numbers, letters, or figures recognition
3. Recommeded system: A system of customization of content suggested based on what you've already seen such as Netflix recommends the next movie to watch, Facebook suggesting groups releated to our interest, Amazon shows prdoucts often purchased togther, youtube suggesting the related videos often watched. 
4. Sentiment analsis: This is analysis of extracting important information from tweets or other data generated by humans about a particular event or character. For example, the post that I tweet in the twitter can be analysed whether the people like or hate the tweets(negative, positive, neutral)
5. Prediction of next words: A system of writing assistance that, as the text in composed, it foresees the next words and suggests them. Such as automatic completion of smartphone or the facebook search bar. This has already been implemented in copilot and notebook python. 
6. A computer vision system that recognize the model and brand of a car by seeing it with a sensor. 

Below are not a Machine learning examples:

1. Search engines: this system in not a machine learning application as it does not improve with experience, and has not got "intelligent", instead have to program explicitly and is totally based on keywords. 

2. A sensor programmed detecting car number plates. As it may seem like that it is releated to the concept of computer vision but there is not involvement of improving with experience, and its performance which are totally based on programming only at the design stage, where it is explicitly calibrated to detect a standard object(the license plate).


In summary, Machine learning are appropriate for: 

1. Fluid environments, where the problem constantly changes and in an unprdictable way. 
2. Complex problems where the traditional approach would be impracticable or would lead to "frankenstein" of code with high maintenance cost. The ML approach can significantly simplify the code and achieve better performance. 
3. Get information about complex problems and discover patterns through large amounts of data. 


# What do I need to do Machine Learning?

Today, every area of science and economy has access to a quatity of data unimaginable even a decade ago. Additionally, there is abundant and inexpensive way of computing capacity which is possible and available through the powerful concept cloud provided by cloud service providers. Uncounted research on machine learning has been taken place and been going on expeditiously in the development of increasignly effective techniques. Similarly, the engineering world creates the ML toolbox by ajusting itself accordingly, and has already developend wonderful frameworks and high-level interfaces to apply ML techniques quickly, which is going towards more user-friednly 'enablers'. 

__Keras__, which is a high-level framework of Pyhon allows to train a deep neural netowork in just 15 lines of code. Cloud providers offer the possibility to host on their machines our ML models, with powerful logic that allows us to buiild a complete AI/ML startup from our laptop.This is a perfect example of the above phonemenon. 

Now let's jump in to the elements that we need to get started. 

## The three elements

__Data__

Data comes from the latin 'Datus', and it's meaning is fact, happened. Building ML programs means exploiting something that has already happened in the past(the data) to discover recurring patterns that suggest how to act in the future. 

ML program is something that we don't write ourselves, but rather something that we understand by observing and processing data. The programmer tries to make the the data do the bulk of the work and engineers combine data with algorthims to grow prorgams. And it is often required to understand that if the data is to do bulk of the work, the quality (and quantity) of the data is strictly necessary for a model to make accurate predictions, or to extract knowledge effectively from them. 

Today there is a large amout of data available to start practicing machine learning and discover its potential. 

Free avaialble data

1. Kaggle 
2. UCI datasets
3. US Government open data
4. EU open data
5. [A very long list from Forbes] (https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/)
6. UK Government open data


__Collect and label your own data__

Data collection is a huge and critical topic, which technical details go outside the scope. So, to collect huge quantities of data we can do through scrapping and other sources. Other resources: [Digital Methods](https://wiki.digitalmethods.net/Dmi/ToolDatabase).

Nowadays, many and more organizations feeling like they are on top of goldmine. As they have years and years of historical data, from machines, conversation logs, transactions, or sensor measurements. There might be temendous value to be unlocked. 

There is a technique of "five-whys", which is a critical thinking process. So, to deal with or get the data part right, let us know our data before performing any actions or tasks or transformations. For detail, go to [github](https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/README-details.md#data)

__know-how__

Doing data science is not simple and will never be if you want to fully understand the mathematical processes behind the elegant models offered by high-level interfaces such as keras. 

The process of developing a DS project consists of: 

1. Business Understanding or domain knowledge
2. Data collection
3. Data preparation
4. Exploratory Data Analysis
5. Modelling
6. Model Evaluation
7. Deploy model


__1. Business understanding:__ This is the first step before going through the analytical process which is basically mean to understand the domain knowledge of the particular company what they are desire to get from the data. Proposing of relevant questions and define objectives for the problem involves in this first step. Data science cycle usually consists of experimenting with solutions in an iterative way by collecting data, shaping the problem and developing ML models. 

__2. Data Collection:__ This is the process of gathering and scraping the data necessary for the project and consists of putting together data sources, building pipelines to pre-process them, and having a place to put them on hold for analysis. 

__3. Data Preparation:__ This process has got two more steps: data cleaning and data pre-processing/data wrangling. This consists of feature engineering(combining existing data in a meaningful way), training models, and evaluating their performance. 

__4. Exploratory Data Analysis:__ EDA is the phase of exploring and visualizing the data to get to know the relationships and features and helps in selection of the features for model building. Formation of hypotheses about the problem that has been defined in the first step and analyze the data by visualizing. 


_ to confirm later:_ Inside EDA: __data exploration__: forming hypothesis about problem by visually analysing the data and __feature engineering__: selecting the important features and construct more meaningful ones using the raw data that you have. 

__5. Modelling:__ This is the phase where the data is used to train a machine learning (or deep learning) model that will be able to predict on never seen data. 

__6. Model Evaluation:__ It consists of evaluating the performance of different models and selecting the best one in the real-life scenario of the problem and use them to make predictions.

__7. Model deployment:__ This is the last step where trained model make available to consumers. 

__8. Data visualization:__ Communicate the findings with key stakeholders using plots and interactive visualizations. 

After all the steps above, regularly monitoring of the performances of the model is required to do and to change as per requirement. 

_Note: It's better to know the agile methodologies in data science_


__Computational Power:__

The third element that is computational power. The most of the time, you will be doing the calculations between matrices while doing experimenting with Machine learning. Similarly, Deep Learning and neural netowrks are by no means of exception for calculations. So, for these calculations, we need high computational power capability computers. In the past, such calculations were made directly using the CPU of computers, which is ofcourse not sufficient to train the most complex networks. 

Fortunately, to overcome the above case, there is GPUs developed which is GRAPHICAL PROCESS UNIT, and its task is exactly to do  matrix calculation. Moreover, due to the internet connection and cloud providers, it is becoming much more convenient to use machines virtually like Amazon, Google and Mircorsoft. 

With a few clicks, files and files of high performance machines can be deployed, and managed in a transparent way especially with on-demand capabilities. This is done all from a dedicated web interface or client, which drastically reduces the cost and complexity of developing complex softwares and solutions. It also allows us to serve solutions quickly, flexibly and without the classic problems of large-scale computing that today's distributed systems require. 

Recently, all large tech companies have started to provide services related to AI and ML. Therefore, data engineer and data scientist are able to achive incredible levels of proudctivity and business through end-to-end platforms that cover the entire development cycle of AI and ML solutions. Some are [AWS Sagemaker](https://aws.amazon.com/it/sagemaker/) and [Azure Machine Learning Studio](https://azure.microsoft.com/it-it/products/machine-learning/).

There are other simple way to experiment or to do data analysis task such google colab or  jupyter notebooks, which are free. 

# Do you really need machine learning? 

Although AI and ML offers immense business and product opportunities, they are not always the recipe for all evils. You can't solve all kind of problem with this. 
Deep learning is a type of artificial intelligence where a computer learns to recognize patterns and make decisions by mimicking how the human brain works. Imagine it as teaching a computer to learn and understand things by itself, just like how we learn from our experiences.

### Simple Explanation with Examples:

1. **Learning Simple to Complex Concepts**:
   - Deep learning models are like a series of building blocks stacked on top of each other, where each block represents a different layer of learning.
   - The first layers learn basic, simple features, like edges in a picture.
   - As you go higher up the layers, the model combines these simple features to recognize more complex patterns, like shapes.
   - Finally, the top layers understand very complex concepts, like recognizing a face in a photograph.

### Example 1: Recognizing a Cat in a Photo

- **Initial Layers**:
  - The first layers might learn to detect edges and lines (simple features).
  - These layers look for things like horizontal, vertical, and diagonal lines.

- **Middle Layers**:
  - These layers combine edges to detect shapes and textures (more complex features).
  - They might recognize patterns like fur, eyes, and whiskers.

- **Final Layers**:
  - The highest layers put all these shapes and textures together to recognize the entire object.
  - At this point, the model can identify that the combination of fur, whiskers, eyes, and specific shapes means it's looking at a cat.

### Example 2: Understanding Spoken Language

- **Initial Layers**:
  - The first layers might learn to detect sounds and phonemes (basic units of sound).
  - They recognize individual sounds like "b," "a," and "t."

- **Middle Layers**:
  - These layers combine sounds to form syllables and words.
  - They might recognize the word "bat."

- **Final Layers**:
  - The highest layers understand the context and meaning of sentences.
  - They can take the word "bat" and understand if it refers to a flying mammal or a piece of sports equipment, based on the surrounding words.

### Summary

Deep learning models start by learning simple elements and gradually build up to understand more complex structures. They are designed in layers, where each layer processes and learns from the output of the previous one, just like how we understand the world by first recognizing small details and then combining them to grasp the bigger picture.


Deep learning requires three elements to function, as we have discuss previously. After taking consideration above all, the next thing is to evaluate opportunites. 

__Evaluate Opportunites__

Having above all the three elements, it is likely that ML and DL can offer incredible possibilites. 

How can we be sure whether if the data have hidden pearls? in general, we can use ML methodds to predict the value(Continuous) or the class(discrete) associcated with a row from having the right amount of data(for example, rows of a database). Or in other words, we can predict the value of a database column. 

_Example_

Let's suppost we have an e-commerce database and contains for each line the record of the purchase of a product. Some of the columns are: "name", "color", "price", "price range". The column "price range" has got a label that identifies if an item has a low price or high, and this range is provided by the company that produces the product. Sometimes, however, the company that supplies the product is careless and forgets to assign a price range to the object. 

__Solving above problem__

The problem is associated with binary classification task which takes as input the price of the object(some features, such as color, do not affect the price in this case) and return output the label of the price range("high" or "low").

In this situation, we can use a deep learning model to predict the "price range" label for products that are missing this information. Similarly, can try more general ML technique and also a deterministic program(traditional programming approach).

With neural network, in which it takes input all the features and returns the value of the price range as output. __But it would probably be an overkill__. We know that data must be cleaned, prepared to be inserted into the network, the network must be trained and adjusted until it reaches satisfactory evaluation metrics. In addition, if tomorrow the requirements of the forecast change(for example if new lable added), all the process need to be done again, and a new network should be trained and evaluted. 

This problem can be solved using just simply Machine Learning generic Linear model that maps the input to the ouptut without the need to introduce "non-linearity". __Linear classifier__(classical ML) can easily solve the problem, and probably get better performance. 

This problem is so simple that the use of heurisic rules such as "if.....then...." could solve it. In fact, we could notice that simply all the objects are a certain N values are classified as "cheap" and those above that N as "very expensive". 

__Deep learning is an extremly powerful tool, and for this reason, we should avoid using it for tasks that are too simple because overengineering is never good. Which means that they are not always the simplest solution, and sometimes the effort that we apply could be useless.__For better understanding the concept click [When not to use Neural Networks](https://medium.datadriveninvestor.com/when-not-to-use-neural-networks-89fb50622429)

Hence, it is higly advisable to ask question yourself first: is my problem quite complex and do I have enough data to make useful machine learning techniques or even deep learning?

The techniques and tools of ML and DL are primarily to solve the business problems. In particular, make sure that data exist, are easily available, are transformable to represent the problem you want to solve. 

Some of the reason for the failure of ML projects. 
1. [unfulfilled expectations](https://www.nature.com/articles/d41586-018-07504-9)
2. [9 Reasons why your machine learning project will fail](https://www.kdnuggets.com/2018/07/why-machine-learning-project-fail.html). Highley recommended to read. 

__No ML without data__

You are within an organization, or have a problem of your own that you want to solve, and you have properly understand what opportunities the ML offers. As from above discussed, we already knew that we need data, computing power and know-how. 

If you cannot get the data for technical, confidentiality or relational reasons, forget about the ML. Without data, we can do nothing, just we can't grow a plant without a seed. So, success of an ML project must be sponsored "from the top", by those have access the data, by those who can retrieve it and make it available. 

of course we don't need unthinkable amount of data(just numbers help), but the less the data we have, the more difficult to generate value through ML projects.

For more visit [medium] on title can yor business use M ML without data? (https://medium.com/swlh/can-your-business-use-machine-learning-without-data-340c59bf9fb0)


### Machine Learning use cases

Below are the use cases for ML, specifically using deep learning: 

a. speech-to-text processing of the phone call

* In the past, this was attempted using preset programs, but only the use of AI has allowed for optimal results. 

* Service is normally offered by the big players: Google, Microsoft, Amazon, etc who have trained the model with millions of sentences; that are available through API

* Once you got the text, we can perform several operations: 

    - a semnatic or liexical analysis, which is translation to text
  
b. recognition of ab object in a photo or video

* an image or a video, receiving in response an on/off when an object is recognized or classification 

* depends on the compatibility between the trained model and the request that is made. 

c. optimization of inbound calls of call center

* company must have previous call history divided in to the individual steps made by the operator; with them, we must be able to build a chain of events that have led to a final solution

* having a large number of these chains at out disposal, we train a model according to ML algorithms, which is necessary to have specific competence at this point to understand which is the most suitable algorithm and the calibration of the parameters necessary to obtain the best result. It is often at this point that the best intentions collapse; it is not enough to feed the data to any model, it is necessary to identify and calibrate the right model.

* Once the model has been trained, made available to call center
* once the call has been received and the first step has been taken, we will probably alrady have the next step available; after the second step, the probability of 'guessin'the next step will be even higher, and so on, optimizing the process. 


__can be applied to all cases where there is sequence of events__

* Actions of a user on the site to predict whether he will buy a product or not
* Analysis of random movements to predict the next event
* Fault/defect analysis to recognize the probability of failure from the current operation
* Analysis of behavior to predict criminal phenomena (Minority report?)

Some of the great resources good to go: 

_Note: this sources is from colabaration with [Derek Snow](https://github.com/firmai)_

_ Research paper with code:https://paperswithcode.com/_

_[9 practical life lessions from Sun Tzu's Art of War](https://blog.tutorabcchinese.com/chinese-culture/life-lessons-from-the-art-of-war)

_ [AI, automation, and the future of work: ten things to solve](https://www.mckinsey.com/featured-insights/future-of-work/ai-automation-and-the-future-of-work-ten-things-to-solve-for)

_ [10 Research backed studying techniques to try in 2022](https://examinedexistence.com/top-10-learning-techniques-ranking-from-best-to-worst/)

_[Self motivation techniques using mind visualization](https://www.mindtosucceed.com/Self-Motivation-Techniques.html)

_[Getting things done](https://en.wikipedia.org/wiki/Getting_Things_Done)

_[implementation intentions](https://www.semanticscholar.org/paper/Implementation-intentions%3A-Strong-effects-of-simple-Gollwitzer/4c216c0ceeef2e2745d113c77a417133c2084dd9?p2df)




### Tools Every Data Scientist should know: A practical Guide

Here, it serves as the basic guidelines to explore the essential tools that require or needed to elevate their data science game, from Python and R to SQL and advanced visulization tools. [Source](https://www.kdnuggets.com/tools-every-data-scientist-should-know-a-practical-guide)


### What is data sciece? 

Simple definition: Data science is a multidisciplinary field that combines knowledge from various disciplines to help businesses make intelligent decisions through data driven analysis. 

Subsections of data science: 

1. Web Scrapping

2. Data exploration and manipulation

3. Data visualization

4. Model building



__Python__

* one of the most frequently utilized languages in data research. 

* Libraries for each category in Python: 

    a. Web Scrapping: 
        * [Beautiful soup](https://beautiful-soup-4.readthedocs.io/en/latest/): Easiest web scraping library
        
        * [Scapy](https://docs.scrapy.org/en/latest/): Advanced web scraping
        
    b. Data Exploration and Manipulation:
        
        * [Pandas](https://pandas.pydata.org/): data manipulation and analysis toolkit
        
        * [NumPy](https://numpy.org/doc/): Supports big multidimensional arrays and mats
        
        
    c. Data Visualization:
    
        * [Matplotlib](https://matplotlib.org/): The core Python plotting library
        
        * [Seaborn](https://seaborn.pydata.org/): A visualization library based on Matplotlib. offers a high level interface for creating attractive statistical graphics. 
        
        * [Plotly](https://plotly.com/python/): Interactive graphing library
        
    d. Model building:
    
        * [Scikit-learn](): the most critical ML library
        
        * [TensorFLow](): Good to apply and scale Deep Learning
        
        * [PyTorch](): Machine Learning library for image processing and NLP applications
        
__R__

* is a potent text analysis tool to address statistical and datat analysis concerns. 

* due to its comprehensive statisitcal power and vast package ecosystem make it quite popular in academia and research

Library: 

a. Web Scraping: 

    * [rvest](https://cran.r-project.org/web/packages/rvest/rvest.pdf): makes web scraping easy be mimicking the exact structure of the web page. 
    
    * [Rcurl](https://cran.r-project.org/web/packages/RCurl/RCurl.pdf): R bindings to the curl lib, allowing for anaything that can be done with the curl itself
    

b. Data Exploration and Manipulation:

    * [dplyr](https://dplyr.tidyverse.org/): It is a grammar of data manipulation offering data manipulation verbs that help make data manipulation easier. 
    
    * [tidyr](https://cran.r-project.org/web/packages/tidyr/tidyr.pdf): makes data more accessible by manually spreading and gathering data
    
    * [Data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) : An extension of data.frame with faster data manipulation capabilites


c. Data Visualization:

    * [ggplot2](https://ggplot2.tidyverse.org/): Application of the grammar of graphics
    
    * [lattice](https://cran.r-project.org/web/packages/lattice/index.html): Better defaults + easy way to create multi-panels-plots
    
    * [plotly](https://plotly.com/r/): It converts graphs created with ggplot2 to interactive, user-driven web-based graphs
    
    
d. Model Building

    * [Caret](https://topepo.github.io/caret/): for creating classification and regression models
    
    * [nnet](https://www.rdocumentation.org/packages/nnet/versions/7.3-19/topics/nnet): offer functions to build neural networks.
    
    * [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf): random forest algorithm-based library for classifcation and regression


__Excel__

* easy way to use for analyzing and visualizing data. 

* has ability handle large data sets makes it helpful for fast data manipulation and analysis. 

* Key functions: 

a. Data Exploration and Manipulation:

    * FILTER: Filters a spectrum of data depending on your defined criteria
    
    * SORT: Sort the elements of a range or array
    
    * VLOOKUP/HLOOKUP ; Find the things in tables or ranges by row or column
    
    * TEXT TO COLUMNS: This will split the content of a cell into mulitple cells.
    
b. Data Visualization:

    * Charts(Bar, Line, Pie): Regular standard chart types to depict data.
    
    * PivotTables : It condenses large datasets and creates interactive summaries
    
    * Conditional Formatting: Displayes which cells fall under a specific rule
    

c. Model building: 

    * AVERAGE, MEDIAN, MODE: calculates central tendencies
    
    * STDEV.P/STDEV.S : works with the dataset to calculate dataset segregration
    
    * LINEST: based on linear regression analysis, statistics for a straingt line 
    that most matches a data set are returned. 
    
    * Regression Analaysis(Data analysis toolpak): uses regression analysis to find the correlations between variables. 
    
    
__SQL_

* Used to interact with relational databases and is needed to store and process data. 

* use to query, update and manage data

* retrieval and analysis. 


Most popular SQL systems:

a. [PostgreSQL](https://www.postgresql.org/): object-relational database system

b. [MYSQL](https://www.mysql.com/): A high level, popular open-source database known for its speed and reliablity

c. [MsSQL(Microsoft SQL Server)](https://www.microsoft.com/en-us/sql-server/): developed RDBMS fully integrated Microsof product with enterprise features. 

d. [Oracle](https://www.oracle.com/): is a multi-model DBMS widely used and combines the best relational model with tree-based storage representation


__Advanced Visualization Tools__

*  complex data can be transformed into vivid, usable insights. 

* helps to create interactive and shareable dashboards that improve, understand, and make the data accessible at the right time.

Tools: 

a. [Power BI](https://www.microsoft.com/en-us/power-platform/products/power-bi/desktop): Business analytics service by Mircosoft that provides interactive visualizations and business intelligence capabilities with an interface simple enough for end users to create their reports and dashboards. 

b. [Tableu](https://www.tableau.com/): A robust data visualization tool that allows users to create interactive and shareable dashboards that give insightful views of the data. It can handles large volumes of data and work well with disparate data sources.

c. [Google Data Studio](https://analytics.google.com/analytics/academy/course/10): is a free parts web-based application that allows you to create dynamic and aesthetic dashboards and reports using data from virtually any source, and other part free, fully customizable, and easy-to-share reports that automatically update using data from your other Google services.



__Cloud Systems__

* are essential because they can scale, increase flexibility and manage big data sets. 

* they offer computational services, tools, and resources to store, process and analyze data at scale with cost optimization and performance effectiveness. 

Platforms:

a. [AWS(Amazon Web Service)](https://aws.amazon.com/): Provides a highly sophisticated and ever-evolving cloud computing platform that includes a range of services such as storage, computation, machine learning, big data analytics etc. 

b. [Google Cloud](https://cloud.google.com/?hl=en): Offers various cloud computing services that run on the same infrastructure Google uses internally for products such as Google Search and Youtubee, including cloud data analytics, data management, and machine learning.

c. [Microsoft Azure](https://azure.microsoft.com/en-us/): offers cloud computing services, includingg virtual machines, databases, AI, and machine learning tools, and DevOps solutions. 

d. [PythonAnywhere](https://www.pythonanywhere.com/): Cloud-based development and hosting environment allowing to run, develop, and host python applications through a web browser without IT staff setting up a server. Ideal for data science and web app developers who want to deplooy their code quickly. 


__LLM's__

* Large Language Models are one of the cutting-edge solutions in AI. 

* can learn and generate text like humans, and they are quite advantageous in a wide range of applications such as NLP, customer Service Automation, Content Generation and so on. Exampes: ChatGPT, Gemini, Claude-3, Microsoft Co-pilot



















Note: For later use: 
For web scrapping: Octaparse, parsehub, dexi.io
From harvard: 
1. CS50’s Introduction to Programming with Python:

The course is designed for students with or without prior programming experience who’d like to learn Python specifically.

Check this out: https://cs50.harvard.edu/python/2022/

2. Statistics:

Learn the fundamentals of statistics required for Data Science

https://edx.org/learn/data-science/harvard-university-data-science-inference-and-modeling

3. Data Pre-Processing:

This will teach you to prepare data and convert it into a format that is easily digestible by machine learning models.

https://edx.org/learn/data-science/harvard-university-data-science-wrangling

4. Data Visualization:

Learn to build visualizations using the ggplot2 library in R, along with the principles of communicating data-driven insights.

https://edx.org/learn/data-visualization/harvard-university-data-science-visualization

5. Machine Learning

This course will teach you the basics of machine learning, techniques to mitigate overfitting, supervised and unsupervised modelling approaches, and recommendation systems.

https://edx.org/learn/machine-learning/harvard-university-data-science-machine-learning

6. Capstone Project:

 Build a Data science project from scratch:

After completing all the above courses take Harvard’s data science capstone project.

Assess your skills in data visualization, statistics, data wrangling and machine learning.

https://edx.org/learn/data-science/harvard-university-data-science-capstone
